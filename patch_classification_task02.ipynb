{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10447112,"sourceType":"datasetVersion","datasetId":6291062}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Flatten, Dense, Input, Conv2D, concatenate, BatchNormalization, MaxPooling2D, UpSampling2D, Concatenate, Dropout, Cropping2D, ZeroPadding2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.metrics import jaccard_score\nimport matplotlib.pyplot as plt\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.metrics import MeanIoU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:51:01.050359Z","iopub.execute_input":"2025-01-14T09:51:01.050634Z","iopub.status.idle":"2025-01-14T09:51:09.163046Z","shell.execute_reply.started":"2025-01-14T09:51:01.050610Z","shell.execute_reply":"2025-01-14T09:51:09.162373Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"TRAIN_IMAGE_DIR = \"/kaggle/input/diabetic-retinopathy-dataset/Daataset_DR/DB0/Images\"\nTRAIN_MASK_DIR = \"/kaggle/input/diabetic-retinopathy-dataset/Daataset_DR/DB0/GroundTruth\"\nTEST_IMAGE_DIR = \"/kaggle/input/diabetic-retinopathy-dataset/Daataset_DR/DB1/Images\"\nTEST_MASK_DIR = \"/kaggle/input/diabetic-retinopathy-dataset/Daataset_DR/DB1/GroundTruth\"\nTRAIN_MASKS = \"/kaggle/working/CombinedMasksTask2\"\nTEST_MASKS = \"/kaggle/working/TestCombinedMasksTask2\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:51:31.683711Z","iopub.execute_input":"2025-01-14T09:51:31.684020Z","iopub.status.idle":"2025-01-14T09:51:31.687873Z","shell.execute_reply.started":"2025-01-14T09:51:31.683996Z","shell.execute_reply":"2025-01-14T09:51:31.686711Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def create_and_save_combined_masks(output_mask_dir, mask_classes, train_image_dir, train_mask_dir):\n    os.makedirs(output_mask_dir, exist_ok=True)\n\n    # Get list of image filenames in the training image directory\n    image_filenames = os.listdir(train_image_dir)\n\n    # Process each imagef\n    for image_filename in tqdm(image_filenames):\n        combined_mask = np.zeros(cv2.imread(os.path.join(train_mask_dir, mask_classes[0], image_filename), cv2.IMREAD_GRAYSCALE).shape, dtype=np.uint8)\n        for subfolder in mask_classes:\n            mask_path = os.path.join(train_mask_dir, subfolder, image_filename)\n            if os.path.exists(mask_path):\n                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n                combined_mask = cv2.bitwise_or(combined_mask, mask)\n\n        output_path = os.path.join(output_mask_dir, image_filename)\n        cv2.imwrite(output_path, combined_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:51:32.771825Z","iopub.execute_input":"2025-01-14T09:51:32.772106Z","iopub.status.idle":"2025-01-14T09:51:32.777352Z","shell.execute_reply.started":"2025-01-14T09:51:32.772084Z","shell.execute_reply":"2025-01-14T09:51:32.776630Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"mask_subfolders = ['H', 'MA']\ncreate_and_save_combined_masks(TRAIN_MASKS, mask_subfolders, TRAIN_IMAGE_DIR, TRAIN_MASK_DIR)\ncreate_and_save_combined_masks(TEST_MASKS, mask_subfolders, TEST_IMAGE_DIR, TEST_MASK_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:51:33.623150Z","iopub.execute_input":"2025-01-14T09:51:33.623454Z","iopub.status.idle":"2025-01-14T09:51:40.221619Z","shell.execute_reply.started":"2025-01-14T09:51:33.623430Z","shell.execute_reply":"2025-01-14T09:51:40.220810Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 130/130 [00:03<00:00, 35.67it/s]\n100%|██████████| 89/89 [00:02<00:00, 30.45it/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def load_data(image_dir, mask_dir):\n    image_filenames = sorted(os.listdir(image_dir))\n    images = []\n    masks = []\n    \n    for filename in tqdm(image_filenames, desc=\"Loading data\"):\n        # Load and resize image\n        image_path = os.path.join(image_dir, filename)\n        image = cv2.imread(image_path)\n        images.append(image)  # Normalize image to [0, 1]\n        \n        mask_path = os.path.join(mask_dir, filename)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        mask = (mask > 0).astype(np.uint8)  # Binarize mask (0 or 1)\n        masks.append(mask)\n    \n    return np.array(images), np.array(masks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:51:41.240029Z","iopub.execute_input":"2025-01-14T09:51:41.240350Z","iopub.status.idle":"2025-01-14T09:51:41.245527Z","shell.execute_reply.started":"2025-01-14T09:51:41.240317Z","shell.execute_reply":"2025-01-14T09:51:41.244669Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"images, masks = load_data(TRAIN_IMAGE_DIR, TRAIN_MASKS)\n\nX_train, X_val, y_train, y_val = train_test_split(images, masks, test_size=0.1, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:51:43.085426Z","iopub.execute_input":"2025-01-14T09:51:43.085708Z","iopub.status.idle":"2025-01-14T09:51:51.583416Z","shell.execute_reply.started":"2025-01-14T09:51:43.085685Z","shell.execute_reply":"2025-01-14T09:51:51.582723Z"}},"outputs":[{"name":"stderr","text":"Loading data: 100%|██████████| 130/130 [00:07<00:00, 16.42it/s]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def data_patches(image_data):\n    # Define patch size\n    patch_size = 16\n\n    # Get the dimensions of the image\n    num_rows, num_cols, num_bands = image_data.shape\n\n    # Initialize an empty list to store patches\n    patches = []\n\n    # Loop over the image in patches of 16x16\n    for row in range(0, num_rows, patch_size):\n        for col in range(0, num_cols, patch_size):\n            # Extract the patch\n            patch = image_data[row:row+patch_size, col:col+patch_size]\n\n            # Check if the patch has the right shape (i.e., it's not on the edge)\n            if patch.shape[:2] == (patch_size, patch_size):\n                patches.append(patch)\n\n    # Convert list of patches to numpy array\n    patches_array = np.array(patches)\n    return patches_array","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:51:51.584431Z","iopub.execute_input":"2025-01-14T09:51:51.584668Z","iopub.status.idle":"2025-01-14T09:51:51.589508Z","shell.execute_reply.started":"2025-01-14T09:51:51.584647Z","shell.execute_reply":"2025-01-14T09:51:51.588555Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_patches = []\n\nfor a in X_train:\n    patches = data_patches(a)\n    train_patches.append(patches)\n\ntrain_patches = np.concatenate(train_patches, axis=0)\ntrain_patches.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:51:57.067117Z","iopub.execute_input":"2025-01-14T09:51:57.067408Z","iopub.status.idle":"2025-01-14T09:51:58.367394Z","shell.execute_reply.started":"2025-01-14T09:51:57.067383Z","shell.execute_reply":"2025-01-14T09:51:58.366676Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(783432, 16, 16, 3)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"validation_patches = []\nfor a in X_val:\n    patches = data_patches(a)\n    validation_patches.append(patches)\n\nvalidation_patches = np.concatenate(validation_patches, axis=0)\nvalidation_patches.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:52:04.054527Z","iopub.execute_input":"2025-01-14T09:52:04.054908Z","iopub.status.idle":"2025-01-14T09:52:04.228973Z","shell.execute_reply.started":"2025-01-14T09:52:04.054878Z","shell.execute_reply":"2025-01-14T09:52:04.228217Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(87048, 16, 16, 3)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"def label_patches2(image):\n    patch_size = 16\n    labeled_patches = []  # List to hold the label of each patch\n\n    # Loop over the image in patches of 8x8\n    for row in range(0, image.shape[0], patch_size):\n        for col in range(0, image.shape[1], patch_size):\n            # Extract the patch\n            patch = image[row:row + patch_size, col:col + patch_size]\n\n            # Check if the patch has the correct shape (i.e., it's not on the edge)\n            if patch.shape[:2] == (patch_size, patch_size):\n                # Check if there is at least one pixel with the value 1\n                if 1 in patch:\n                    labeled_patches.append(1)\n                else:\n                    labeled_patches.append(0)\n\n    return np.array(labeled_patches)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:52:11.687311Z","iopub.execute_input":"2025-01-14T09:52:11.687608Z","iopub.status.idle":"2025-01-14T09:52:11.692453Z","shell.execute_reply.started":"2025-01-14T09:52:11.687585Z","shell.execute_reply":"2025-01-14T09:52:11.691409Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"train_labels = []\n\nfor a in y_train:\n    labels = label_patches2(a)\n    train_labels.extend(labels)\n\ntrain_labels = np.array(train_labels)\ntrain_labels.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:52:18.844490Z","iopub.execute_input":"2025-01-14T09:52:18.844827Z","iopub.status.idle":"2025-01-14T09:52:23.534507Z","shell.execute_reply.started":"2025-01-14T09:52:18.844799Z","shell.execute_reply":"2025-01-14T09:52:23.533781Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(783432,)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"validation_labels = []\n\nfor a in y_val:\n    labels = label_patches2(a)\n    validation_labels.extend(labels)\n\nvalidation_labels = np.array(validation_labels)\nvalidation_labels.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:52:24.785866Z","iopub.execute_input":"2025-01-14T09:52:24.786176Z","iopub.status.idle":"2025-01-14T09:52:25.307106Z","shell.execute_reply.started":"2025-01-14T09:52:24.786156Z","shell.execute_reply":"2025-01-14T09:52:25.306324Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(87048,)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"def build_model():\n    model = Sequential()\n    model.add(Conv2D(128, (3, 3), activation='relu', padding='same', input_shape=(16, 16, 3)))\n    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(1024, (3, 3), activation='relu', padding='same'))\n    model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(2, activation='sigmoid'))\n\n    # Compile the model\n    model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n    # Model summary to check structure\n    return model\nmodel = build_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:52:33.072845Z","iopub.execute_input":"2025-01-14T09:52:33.073140Z","iopub.status.idle":"2025-01-14T09:52:33.838515Z","shell.execute_reply.started":"2025-01-14T09:52:33.073118Z","shell.execute_reply":"2025-01-14T09:52:33.837555Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\n\n# Setup the ModelCheckpoint callback to save the best model based on validation accuracy\ncheckpoint = ModelCheckpoint(\n    'best_model.keras',  # Path where the model is saved\n    monitor='val_accuracy',  # Monitor validation accuracy\n    save_best_only=True,  # Only save the model if 'val_accuracy' has improved\n    mode='max',  # 'max' because we want to maximize validation accuracy\n    verbose=1  # Optional: provides detailed logging about the saved models\n)\n\n# Fit the model using the previously defined training and validation data\nhistory2 = model.fit(\n    train_patches, \n    train_labels, \n    validation_data=(validation_patches, validation_labels),\n    epochs=50,\n    batch_size=2048,\n    callbacks=[checkpoint]  # Include the checkpoint in the callbacks\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:52:49.836419Z","iopub.execute_input":"2025-01-14T09:52:49.836764Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - accuracy: 0.9820 - loss: 0.0767\nEpoch 1: val_accuracy improved from -inf to 0.99045, saving model to best_model.keras\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 383ms/step - accuracy: 0.9820 - loss: 0.0766 - val_accuracy: 0.9905 - val_loss: 0.0564\nEpoch 2/50\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.9952 - loss: 0.0352\nEpoch 2: val_accuracy did not improve from 0.99045\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 265ms/step - accuracy: 0.9952 - loss: 0.0352 - val_accuracy: 0.9905 - val_loss: 0.0620\nEpoch 3/50\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.9952 - loss: 0.0339\nEpoch 3: val_accuracy did not improve from 0.99045\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 265ms/step - accuracy: 0.9952 - loss: 0.0339 - val_accuracy: 0.9905 - val_loss: 0.0523\nEpoch 4/50\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.9954 - loss: 0.0288\nEpoch 4: val_accuracy improved from 0.99045 to 0.99055, saving model to best_model.keras\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 267ms/step - accuracy: 0.9954 - loss: 0.0288 - val_accuracy: 0.9905 - val_loss: 0.0461\nEpoch 5/50\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.9954 - loss: 0.0275\nEpoch 5: val_accuracy improved from 0.99055 to 0.99083, saving model to best_model.keras\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 267ms/step - accuracy: 0.9954 - loss: 0.0275 - val_accuracy: 0.9908 - val_loss: 0.0447\nEpoch 6/50\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.9955 - loss: 0.0265\nEpoch 6: val_accuracy improved from 0.99083 to 0.99084, saving model to best_model.keras\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 267ms/step - accuracy: 0.9955 - loss: 0.0265 - val_accuracy: 0.9908 - val_loss: 0.0471\nEpoch 7/50\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.9956 - loss: 0.0257\nEpoch 7: val_accuracy improved from 0.99084 to 0.99094, saving model to best_model.keras\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 267ms/step - accuracy: 0.9956 - loss: 0.0257 - val_accuracy: 0.9909 - val_loss: 0.0471\nEpoch 8/50\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.9957 - loss: 0.0243\nEpoch 8: val_accuracy improved from 0.99094 to 0.99110, saving model to best_model.keras\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 267ms/step - accuracy: 0.9957 - loss: 0.0243 - val_accuracy: 0.9911 - val_loss: 0.0455\nEpoch 9/50\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.9958 - loss: 0.0235\nEpoch 9: val_accuracy improved from 0.99110 to 0.99117, saving model to best_model.keras\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 267ms/step - accuracy: 0.9958 - loss: 0.0235 - val_accuracy: 0.9912 - val_loss: 0.0434\nEpoch 10/50\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.9959 - loss: 0.0230\nEpoch 10: val_accuracy did not improve from 0.99117\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 265ms/step - accuracy: 0.9959 - loss: 0.0230 - val_accuracy: 0.9911 - val_loss: 0.0424\nEpoch 11/50\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.9957 - loss: 0.0227\nEpoch 11: val_accuracy did not improve from 0.99117\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 265ms/step - accuracy: 0.9957 - loss: 0.0227 - val_accuracy: 0.9912 - val_loss: 0.0412\nEpoch 12/50\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.9958 - loss: 0.0217\nEpoch 12: val_accuracy did not improve from 0.99117\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 265ms/step - accuracy: 0.9958 - loss: 0.0217 - val_accuracy: 0.9911 - val_loss: 0.0390\nEpoch 13/50\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.9959 - loss: 0.0209\nEpoch 13: val_accuracy did not improve from 0.99117\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 265ms/step - accuracy: 0.9959 - loss: 0.0209 - val_accuracy: 0.9911 - val_loss: 0.0425\nEpoch 14/50\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.9960 - loss: 0.0200\nEpoch 14: val_accuracy did not improve from 0.99117\n\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 265ms/step - accuracy: 0.9960 - loss: 0.0200 - val_accuracy: 0.9911 - val_loss: 0.0379\nEpoch 15/50\n\u001b[1m101/383\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 257ms/step - accuracy: 0.9959 - loss: 0.0198","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"IMAGE_DIR = \"/kaggle/input/diabetic-retinopathy-dataset/Daataset DR/DB1/Images\"\nMASK_DIR = \"/kaggle/working/TestCombinedMasksTask2\"\ndef load_data(image_dir, mask_dir):\n    image_filenames = sorted(os.listdir(image_dir))\n    images = []\n    masks = []\n    \n    for filename in tqdm(image_filenames, desc=\"Loading data\"):\n        # Load and resize image\n        image_path = os.path.join(image_dir, filename)\n        image = cv2.imread(image_path)\n        images.append(image)\n        \n        mask_path = os.path.join(mask_dir, filename)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        mask = (mask > 0).astype(np.uint8)  # Binarize mask (0 or 1)\n        masks.append(mask)\n    \n    return np.array(images), np.array(masks)\n\nX_test, y_test = load_data(IMAGE_DIR, MASK_DIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_patches = []\n\nfor a in X_test:\n    patches = data_patches(a)\n    test_patches.append(patches)\n\ntest_patches = np.concatenate(test_patches, axis=0)\ntest_patches.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_labels = []\n\nfor a in y_test:\n    labels = label_patches2(a)\n    test_labels.extend(labels)\n\ntest_labels = np.array(test_labels)\ntest_labels.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate(test_patches,test_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = model.predict(test_patches)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict = np.argmax(predictions, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred = []\nfor x in predict:\n  pred.append(np.full((16, 16), x))\npred = np.array(pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred2 = pred.reshape(89, 6696, 16,16)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_masks = []\nfor a in pred2:\n    patches_per_row = 93\n    patches_per_col = len(a) // patches_per_row\n    image = np.concatenate([np.concatenate(a[i:i+patches_per_row], axis=1) for i in range(0, len(a), patches_per_row)], axis=0)\n    predicted_masks.append(image)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming 'predicted_masks' is your list or array of images\nnum_images = 89\nrows = 18\ncols = 5\n\nfig, axes = plt.subplots(rows, cols, figsize=(15, 54))  # Adjust figsize to your screen/display size\naxes = axes.flatten()  # Flatten the 2D array of axes to simplify the looping\n\n# Loop over all of the positions in the grid\nfor i in range(rows * cols):\n    if i < num_images:\n        # Display image\n        axes[i].imshow(predicted_masks[i], cmap='gray')  # Assuming masks are grayscale\n        axes[i].axis('off')  # Turn off axis numbering and ticks\n    else:\n        axes[i].axis('off')  # Make sure empty plots also have no axes\n\nplt.tight_layout()  # Optional, improves spacing between plots\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.image as mpimg\n\ntest_image_dir = \"/kaggle/input/diabetic-retinopathy-dataset/Daataset DR/DB1/Images\"\nground_truth_dir = \"/kaggle/working/TestCombinedMasksTask2\"\nimage_files = sorted(os.listdir(test_image_dir))\nground_truth_files = sorted(os.listdir(ground_truth_dir))\n\nassert len(image_files) == len(ground_truth_files) == 89, \"Mismatch in number of files\"\n\nnum_images = len(image_files)\nfig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))\n\nfor i, image_file in enumerate(image_files):\n    # Read the image and the ground truth mask\n    img_path = os.path.join(test_image_dir, image_file)\n    gt_path = os.path.join(ground_truth_dir, ground_truth_files[i])\n\n    image = mpimg.imread(img_path)\n    ground_truth = mpimg.imread(gt_path)\n    predicted_mask = predicted_masks[i]\n\n    # Plotting\n    axes[i, 0].imshow(image)\n    axes[i, 0].set_title('Original Image')\n    axes[i, 0].axis('off')\n\n    axes[i, 1].imshow(ground_truth)\n    axes[i, 1].set_title('Ground Truth Mask')\n    axes[i, 1].axis('off')\n\n    axes[i, 2].imshow(predicted_mask)\n    axes[i, 2].set_title('Predicted Mask')\n    axes[i, 2].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"padded_masks = np.array([np.pad(mask, pad_width=((0, 0), (0, 12)), mode='constant', constant_values=0) for mask in predicted_masks])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def iou_score(y_true, y_pred):\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 1.0  # To handle cases with no ground truth or predicted objects\n    else:\n        return intersection / union","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def iou_score(y_true, y_pred):\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 1.0  # To handle cases with no ground truth or predicted objects\n    else:\n        return intersection / union","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ***Task2 END***","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}